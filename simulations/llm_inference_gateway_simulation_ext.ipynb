{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArYDriKfYS1b",
        "outputId": "75f11fee-93d9-4f87-b81a-72a4b7b9189d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: simpy in /usr/local/lib/python3.10/dist-packages (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install simpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "On-rPqlDYW75"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "from collections import defaultdict\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import time\n",
        "import torch\n",
        "import simpy\n",
        "\n",
        "import json\n",
        "\n",
        "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tkinter as tk\n",
        "from PIL import ImageTk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNcmX2C0Ykaz"
      },
      "source": [
        "# Modelling the Prefill and Decode Latencies\n",
        "## Prefill Latency\n",
        "Assuming a batch size of $B_p$, the number of tokens in a batch is:\n",
        "$$\n",
        "t_p = \\Sigma_i^{B_p} l_i\n",
        "$$\n",
        "and sum of squares is:\n",
        "$$\n",
        "t'_p = \\Sigma_i^{B_p} l_i^2\n",
        "$$\n",
        "where there are $l_i$ tokens in ith batch.\n",
        "Since we dont have KV Cache available yet, the time complexity for this step goes as $O(n^2)$ and saturates at low $t_p$ i.e.\n",
        "$$\n",
        "T_{Prefill} = \\text{max}(C_{\\text{min}}, C_0 + C_1  t_p + C_2 t'_p)\n",
        "$$\n",
        "where the constants $C_{\\text{min}},$ $C_0$, $C_1$ and $C_2$ depends on various factors including the model architechture.\n",
        "\n",
        "## Decode Latency\n",
        "In this step since the KV Cache is available, the time complexity goes as $O(n)$.\n",
        "Assuming a batch size of $B_d$, the number of tokens in a batch is:\n",
        "$$\n",
        "t_d = \\Sigma_i^{B_d} l_i\n",
        "$$\n",
        "where there are $l_i$ tokens in ith batch.\n",
        "$$\n",
        "T_{Decode} = C_3 + C_4 t_d\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpmGL9lTZGa-"
      },
      "source": [
        "# Continous Batching Algorithm\n",
        "Below we will simulate continous batching where the scheduling decision is made before each forward pass. Any given batch comprises either purely of prefill phase, or only decode phase. vLLM and TGI both provide multiple input arguments to control how to schedule one vs the other. In the simulation below we will use the following logic:\n",
        "Three processes are running in parallel.\n",
        "\n",
        "* A process that accepts incoming requests and puts it in a **prefill queue**\n",
        "* A process that either prefills or decodes.\n",
        "  * If both **decode queue** and **prefill queue** are empty it waits for the next request to arrive.\n",
        "  * If **prefill queue** is not empty and we have enough memory to store kv cache for the new sequence and the number of sequence in **decode queue** is less than **MAX_NUM_SEQ** then prefill and put the seq in **decode queue**\n",
        "  * If we do not have enough memory **evict** the newest sequence from **decode queue** put it back to prefill queue (with the new tokens generated). If not skip this step.\n",
        "  *  **decode**: take every request in  **decode queue** and decode one step\n",
        "  \n",
        "* A process to fetch results from the **decoded queue**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "YyxvINDwYhSK"
      },
      "outputs": [],
      "source": [
        "import simpy\n",
        "import random\n",
        "import numpy\n",
        "\n",
        "\n",
        "NO_OF_ACTORS = 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "PREFILL_LATENCY_CONST_2 = 0\n",
        "PREFILL_LATENCY_CONST_1 = 0.00006769375513\n",
        "PREFILL_LATENCY_CONST_0 = 0.01969\n",
        "PREFILL_LATENCY_CONST_MIN = 0.04\n",
        "\n",
        "DECODE_LATENCY_CONST_1 = 0.000000766\n",
        "DECODE_LATENCY_CONST_0 = 0.014\n",
        "TOKENIZE_LATENCY_CONST =   0\n",
        "\n",
        "MAX_NUM_BATCH_TOKENS = 512 # in prefill\n",
        "\n",
        "TOTAL_NUM_GPU_BLOCKS = 2810\n",
        "NUMBER_OF_TOKENS_PER_BLOCK = 16\n",
        "MAX_NUM_TOKENS_ALLOWED = TOTAL_NUM_GPU_BLOCKS*NUMBER_OF_TOKENS_PER_BLOCK - MAX_NUM_BATCH_TOKENS # in kv cache\n",
        "MAX_GPU_MEMORY_PERC_BEFORE_RECOMPUTE = 0.9\n",
        "MAX_GPU_MEMORY_PERC_BEFORE_RECOMPUTE_NON_CRITICAL = 0.8\n",
        "MAX_NUM_SEQ = 256\n",
        "\n",
        "\n",
        "# size of each lora in units of KV Cache\n",
        "LORA_DICT = {\"tweet\" : 1600, \"sql\" : 1600, \"dummy-1\" : 0, \"dummy-2\" : 0}\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYb0aBkWxrON"
      },
      "source": [
        "Above we are assuming the following behavior of decode and prefill latencies. They are based on A100, 40 GB, llama-3 architechture on vllm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))  # 1 row, 2 columns, size 12x5 inches\n",
        "\n",
        "\n",
        "# Create a range of x values\n",
        "x = [x for x in range(1025)]\n",
        "\n",
        "x1 = [x for x in range(MAX_NUM_TOKENS_ALLOWED)]\n",
        "\n",
        "# First plot\n",
        "ax1.plot(x, [max(PREFILL_LATENCY_CONST_MIN, PREFILL_LATENCY_CONST_0 + PREFILL_LATENCY_CONST_1*t) for t in x],)\n",
        "ax1.set_xlabel('prompt token count')\n",
        "ax1.set_ylabel('latency (sec)')\n",
        "ax1.set_title('Prefill Latency (batch size = 1)')\n",
        "\n",
        "ax1.grid(True)\n",
        "\n",
        "# Second plot\n",
        "ax2.plot(x1, [ DECODE_LATENCY_CONST_0 + DECODE_LATENCY_CONST_1*t for t in x1],)\n",
        "ax2.set_xlabel('number of tokens in kv cache')\n",
        "#ax2.set_ylabel('latency (sec)')\n",
        "ax2.set_title(f'Decode Latency of 1 iteration')\n",
        "\n",
        "ax2.grid(True)\n",
        "\n",
        "# Adjust the layout to avoid overlap\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "Kbesri83yqu4",
        "outputId": "01ee372a-6fd4-4df1-a87e-7c88cff40db9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy7lKWM_Y-ji"
      },
      "source": [
        "LLMActor is the simulation of the model server. It has\n",
        "\n",
        "\n",
        "*   Prefill Store: A queue where all to be prefilled requests are stored\n",
        "*   Decode Store: A queue where all requests in KV Cache are to be stored\n",
        "*   Decoded Store: A queue where the outputs are stored\n",
        "*   Recompute Store: A queue where evicted requests are store\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "dLg26qNEYvsU"
      },
      "outputs": [],
      "source": [
        "import simpy\n",
        "import numpy as np\n",
        "import queue\n",
        "\n",
        "class LLMActor(object):\n",
        "    \"\"\"This class represents the propagation through an LLM Inference Actor managing multiple stores with Request objects.\"\"\"\n",
        "\n",
        "    def __init__(self, env, number_of_actors=1, actorId = 0):\n",
        "        \"\"\"Initialize the simulation environment and stores.\"\"\"\n",
        "        self.env = env\n",
        "        self.prefill_store = simpy.Store(env)\n",
        "        self.decode_store = simpy.FilterStore(env)\n",
        "        self.decoded_store = simpy.Store(env)\n",
        "        self.recompute_store = simpy.PriorityStore(env)\n",
        "        self.actor = simpy.Resource(env, capacity=number_of_actors)  # Now dynamically set capacity\n",
        "        self.user = simpy.Resource(env, capacity = 1)\n",
        "        self.id = actorId\n",
        "        self.lora_loaded = set()\n",
        "        self.max_num_tokens_allowed = MAX_NUM_TOKENS_ALLOWED\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def get_num_tokens(self, store, include_remaining=True):\n",
        "        \"\"\"Calculate the total number of tokens in a given store, optionally including remaining output tokens.\"\"\"\n",
        "        if include_remaining:\n",
        "            return np.sum([x.input_size + x.output_size - x.output_size_remaining for x in store.items])\n",
        "        return np.sum([x.input_size for x in store.items])\n",
        "\n",
        "    def get_num_tokens_in_decode(self):\n",
        "        \"\"\"Return the number of total tokens currently in the decode store.\"\"\"\n",
        "        return self.get_num_tokens(self.decode_store)\n",
        "\n",
        "    def get_num_prompt_tokens_in_decode(self):\n",
        "        \"\"\"Return the number of input tokens currently in the decode store.\"\"\"\n",
        "        return np.sum([x.input_size for x in self.decode_store.items])\n",
        "\n",
        "    def get_num_gen_tokens_in_decode(self):\n",
        "        \"\"\"Return the number of output tokens remaining to be generated in the decode store.\"\"\"\n",
        "        return np.sum([x.output_size - x.output_size_remaining for x in self.decode_store.items])\n",
        "\n",
        "    def get_num_gen_tokens_in_decoded(self):\n",
        "        \"\"\"Return the number of output tokens remaining to be generated in the decode store.\"\"\"\n",
        "        return np.sum([x.output_size - x.output_size_remaining for x in self.decoded_store.items])\n",
        "\n",
        "    def get_num_prompt_tokens_in_decoded(self):\n",
        "        \"\"\"Return the number of output tokens remaining to be generated in the decode store.\"\"\"\n",
        "        return np.sum([x.input_size for x in self.decoded_store.items])\n",
        "\n",
        "    def get_queue_size(self, store):\n",
        "        \"\"\"Return the current queue size of a given store.\"\"\"\n",
        "        return len(store.items)\n",
        "\n",
        "    def get_decode_queue_size(self):\n",
        "      return self.get_queue_size(self.decode_store)\n",
        "\n",
        "    def get_prefill_queue_size(self):\n",
        "      return self.get_queue_size(self.prefill_store)\n",
        "\n",
        "    def get_recompute_queue_size(self):\n",
        "      return self.get_queue_size(self.recompute_store)\n",
        "\n",
        "    def get_decoded_queue_size(self):\n",
        "      return self.get_queue_size(self.decoded_store)\n",
        "\n",
        "    def get_min_expected_num_tokens_in_kvcache_after_prefill(self):\n",
        "        \"\"\"Calculate the minimum expected number of tokens in the key-value cache after prefill.\"\"\"\n",
        "        num_tokens_decode = self.get_num_tokens_in_decode()\n",
        "        if self.get_queue_size(self.recompute_store) > 0:\n",
        "            item = self.recompute_store.items[0].item\n",
        "            return num_tokens_decode + item.input_size + item.output_size - item.output_size_remaining\n",
        "        elif self.get_queue_size(self.prefill_store) > 0:\n",
        "            item = self.prefill_store.items[0]\n",
        "            return num_tokens_decode + item.input_size + item.output_size - item.output_size_remaining\n",
        "\n",
        "        return num_tokens_decode\n",
        "\n",
        "\n",
        "    def get_expected_num_tokens_in_kvcache_after_decode(self):\n",
        "          return self.get_decode_queue_size() + self.get_num_tokens_in_decode()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsaZgn_lb5mH"
      },
      "source": [
        "The request class stores a request with its attributes like input size, output size etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HVrAj75zZaE4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import simpy\n",
        "import numpy as np\n",
        "\n",
        "class Request:\n",
        "    def __init__(self, id, arrival_time, input_size, output_size, lora):\n",
        "        self.id = id\n",
        "        self.arrival_time = arrival_time\n",
        "        self.input_size = input_size\n",
        "\n",
        "        self.start_prefill_time = None\n",
        "        self.prefill_time = None\n",
        "        self.tokens_in_kv_cache_at_start_of_decode = None\n",
        "        self.start_decode_time = None\n",
        "        self.end_first_token_decode_time = None\n",
        "        self.end_decode_time = None\n",
        "        self.output_size = output_size\n",
        "        self.output_size_remaining = output_size\n",
        "        self.recompute_count = 0\n",
        "        self.target_pod = None\n",
        "        self.target_latency = np.inf\n",
        "        self.tokens_in_kv_cache_at_start_of_decode = None\n",
        "\n",
        "        self.queue_size_before_prefill = None\n",
        "\n",
        "\n",
        "        self.estimated_latency = 0\n",
        "\n",
        "        self.lora = lora\n",
        "\n",
        "\n",
        "\n",
        "def create_request(id, time, input_size, output_size, lora = None):\n",
        "    \"\"\"Creates a new request with given parameters.\"\"\"\n",
        "    return Request(id, time, input_size, output_size, lora)\n",
        "\n",
        "def determine_size(distribution_mean, distribution_std, sizes_dict=None, id=None):\n",
        "    \"\"\"Determines the size of a request component based on a dictionary, normal distribution, or fixed size.\"\"\"\n",
        "    if sizes_dict and id in sizes_dict:\n",
        "        return max(1, sizes_dict[id])\n",
        "    return max(1.0, np.round(np.abs(np.random.normal(distribution_mean, distribution_std, 1))).astype(int)[0])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGtpitbOZju5"
      },
      "source": [
        "Metrics can run as another process to emit metrics periodically"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "3DOjOVHGZeLM"
      },
      "outputs": [],
      "source": [
        "def metrics(env, llmactor):\n",
        "  while True:\n",
        "      yield env.timeout(10)\n",
        "      cur_time = env.now\n",
        "      num_of_prompt_tokens = llmactor.get_num_prompt_tokens_in_decode() + llmactor.get_num_prompt_tokens_in_decoded()\n",
        "      num_of_gen_tokens = llmactor.get_num_gen_tokens_in_decode() + llmactor.get_num_gen_tokens_in_decoded()\n",
        "      running_req = llmactor.get_decode_queue_size()\n",
        "      pending_req = llmactor.get_prefill_queue_size()\n",
        "      gpu_kv_cache_usage = llmactor.get_num_tokens_in_decode()/llmactor.max_num_tokens_allowed * 100\n",
        "      print(f'llmactor {llmactor.id} Avg prompt throughput: {num_of_prompt_tokens/cur_time} tokens/s, Avg generation throughput: {num_of_gen_tokens/cur_time}, Running: {running_req} reqs, Pending: {pending_req} reqs, GPU KV cache usage: {gpu_kv_cache_usage}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0ZQyqHVZtgo"
      },
      "source": [
        "The continous batching algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qVVKPVQlZs0K"
      },
      "outputs": [],
      "source": [
        "import simpy\n",
        "import numpy as np\n",
        "\n",
        "def should_process_prefill_or_recompute(llmactor, env):\n",
        "    \"\"\"Check if the system should process prefill or recompute based on queue sizes and memory constraints.\"\"\"\n",
        "    return (llmactor.get_prefill_queue_size() > 0 or llmactor.get_recompute_queue_size() > 0) and \\\n",
        "           (llmactor.get_decode_queue_size() < MAX_NUM_SEQ) and \\\n",
        "           (llmactor.get_min_expected_num_tokens_in_kvcache_after_prefill() / (llmactor.max_num_tokens_allowed + 0.0) < MAX_GPU_MEMORY_PERC_BEFORE_RECOMPUTE)\n",
        "\n",
        "def fetch_prefill_items(llmactor, env,  ):\n",
        "    \"\"\"Fetch items to prefill if there is memory either from recompute (p0) or from prefill (p1)\"\"\"\n",
        "    items_to_prefill = []\n",
        "    prefill_batch_size = 0\n",
        "    num_new_seq = 0\n",
        "\n",
        "    while llmactor.get_recompute_queue_size() > 0:\n",
        "        oldest_item = llmactor.recompute_store.items[0].item\n",
        "        oldest_item_len = oldest_item.input_size + oldest_item.output_size - oldest_item.output_size_remaining\n",
        "\n",
        "        if any([\n",
        "            llmactor.get_decode_queue_size() + num_new_seq + 1 > MAX_NUM_SEQ,\n",
        "            prefill_batch_size + oldest_item_len > MAX_NUM_BATCH_TOKENS,\n",
        "            (prefill_batch_size + num_new_seq + llmactor.get_num_tokens_in_decode()) / (llmactor.max_num_tokens_allowed + 0.0) >= MAX_GPU_MEMORY_PERC_BEFORE_RECOMPUTE\n",
        "        ]):\n",
        "            break\n",
        "\n",
        "        prefill_batch_size += oldest_item_len\n",
        "        num_new_seq += 1\n",
        "        msg = yield llmactor.recompute_store.get()\n",
        "        items_to_prefill.append(msg.item)\n",
        "\n",
        "    while llmactor.get_prefill_queue_size() > 0:\n",
        "        oldest_item = llmactor.prefill_store.items[0]\n",
        "        oldest_item_len = oldest_item.input_size + oldest_item.output_size - oldest_item.output_size_remaining\n",
        "\n",
        "        if any([\n",
        "            llmactor.get_decode_queue_size() + num_new_seq + 1 > MAX_NUM_SEQ,\n",
        "            prefill_batch_size + oldest_item_len > MAX_NUM_BATCH_TOKENS,\n",
        "            (prefill_batch_size + num_new_seq + llmactor.get_num_tokens_in_decode()) / (llmactor.max_num_tokens_allowed + 0.0) >= MAX_GPU_MEMORY_PERC_BEFORE_RECOMPUTE\n",
        "        ]):\n",
        "            break\n",
        "\n",
        "        prefill_batch_size += oldest_item_len\n",
        "        num_new_seq += 1\n",
        "        msg = yield llmactor.prefill_store.get()\n",
        "        items_to_prefill.append(msg)\n",
        "\n",
        "    #if (len(items_to_prefill) == 0) and llmactor.get_prefill_queue_size() > 0:\n",
        "    #    msg = yield llmactor.prefill_store.get()\n",
        "    #    items_to_prefill.append(msg)\n",
        "\n",
        "    return items_to_prefill\n",
        "\n",
        "def process_prefill_items( llmactor, env, items_to_prefill, req_dict_prefill, req_dict, logging = False):\n",
        "    \"\"\"Process prefill items, updating times and managing item states.\"\"\"\n",
        "    prefill_len = np.sum([x.input_size + x.output_size - x.output_size_remaining for x in items_to_prefill])\n",
        "    prefill_delay = calculate_prefill_delay(prefill_len, len(items_to_prefill), TOKENIZE_LATENCY_CONST, PREFILL_LATENCY_CONST_2, PREFILL_LATENCY_CONST_1 , PREFILL_LATENCY_CONST_0, PREFILL_LATENCY_CONST_MIN)\n",
        "\n",
        "\n",
        "    for item in items_to_prefill:\n",
        "        #lora stuff\n",
        "        if item.lora is not None:\n",
        "              if item.lora not in llmactor.lora_loaded:\n",
        "                llmactor.lora_loaded.add(item.lora)\n",
        "                llmactor.max_num_tokens_allowed -=  LORA_DICT[item.lora]\n",
        "\n",
        "        if item.start_prefill_time is None:\n",
        "            item.start_prefill_time = env.now\n",
        "            item.end_prefill_time = item.start_prefill_time + prefill_delay\n",
        "        item.end_decode_time = llmactor.env.now + prefill_delay\n",
        "        item.output_size_remaining -= 1\n",
        "\n",
        "        if item.output_size_remaining == 0:\n",
        "            llmactor.decoded_store.put(item)\n",
        "        else:\n",
        "            llmactor.decode_store.put(item)\n",
        "            if item.output_size_remaining <= 0:\n",
        "              if logging:\n",
        "                print(f'llmactor {llmactor.id} {item.id} item.output_size_remaining {item.output_size_remaining}')\n",
        "              assert item.output_size_remaining > 0\n",
        "        req_dict_prefill[item.id] = item\n",
        "        req_dict[item.id] = item\n",
        "    return prefill_delay\n",
        "\n",
        "def should_recompute(llmactor, env):\n",
        "    \"\"\"Determine if items should be moved to recompute based on memory usage.\"\"\"\n",
        "    return llmactor.get_expected_num_tokens_in_kvcache_after_decode() / (llmactor.max_num_tokens_allowed + 0.0) > MAX_GPU_MEMORY_PERC_BEFORE_RECOMPUTE\n",
        "\n",
        "def remove_from_decode_store(llmactor, env, req_dict_prefill, req_dict, logging = False):\n",
        "    \"\"\"Manage the recomputation of items based on priority and conditions.\"\"\"\n",
        "    while should_recompute(llmactor, env):\n",
        "        if llmactor.get_decode_queue_size() > 0:\n",
        "            newest_decode_item_id = llmactor.decode_store.items[-1].id  # newest item goes to recompute\n",
        "            if logging:\n",
        "              print(f'llmactor {llmactor.id} removing from decode store sequence {newest_decode_item_id}')\n",
        "            req_dict[newest_decode_item_id].recompute_count += 1\n",
        "\n",
        "            newest_decode_item = yield llmactor.decode_store.get(lambda req: req.id == newest_decode_item_id)\n",
        "            llmactor.recompute_store.put(simpy.PriorityItem(item=newest_decode_item, priority=newest_decode_item_id))\n",
        "\n",
        "def decode_items(llmactor, env, req_dict_prefill, req_dict, logging=False):\n",
        "    \"\"\"Process decoding of items, handling them appropriately based on their remaining output size.\"\"\"\n",
        "    num_items_to_decode = llmactor.get_decode_queue_size()\n",
        "    before_decoding_token_count = llmactor.get_num_tokens_in_decode()\n",
        "    temp_items = []\n",
        "    decode_delay = calculate_decode_delay(before_decoding_token_count, num_items_to_decode, TOKENIZE_LATENCY_CONST, DECODE_LATENCY_CONST_1, DECODE_LATENCY_CONST_0)\n",
        "    if logging:\n",
        "      print(f'llmactor {llmactor.id} Decoding sequences {[x.id for x in llmactor.decode_store.items]} items with delay {decode_delay}')\n",
        "\n",
        "    for _ in range(num_items_to_decode):\n",
        "        msg = yield llmactor.decode_store.get()\n",
        "        if msg.output_size_remaining == msg.output_size-1:\n",
        "          msg.start_decode_time = env.now\n",
        "          msg.tokens_in_kv_cache_at_start_of_decode = before_decoding_token_count\n",
        "        msg.output_size_remaining -= 1\n",
        "        if msg.output_size_remaining < 0:\n",
        "            raise ValueError(f'Output size remaining negative for {msg.id}')\n",
        "\n",
        "        temp_items.append(msg)\n",
        "        req_dict_prefill[msg.id] = msg\n",
        "        req_dict[msg.id] = msg\n",
        "\n",
        "\n",
        "\n",
        "    for item in temp_items:\n",
        "        if item.output_size_remaining == 0:\n",
        "            item.end_decode_time = env.now + decode_delay\n",
        "\n",
        "            llmactor.decoded_store.put(item)\n",
        "        else:\n",
        "            item.end_decode_time = env.now + decode_delay\n",
        "            llmactor.decode_store.put(item)\n",
        "\n",
        "    return decode_delay\n",
        "\n",
        "def calculate_decode_delay(token_count, num_items_to_decode, TOKENIZE_LATENCY_CONST, DECODE_LATENCY_CONST_1=0.01, DECODE_LATENCY_CONST_0=1):\n",
        "    \"\"\"Calculate delay based on the token count and latency constants.\"\"\"\n",
        "    return token_count * DECODE_LATENCY_CONST_1 + DECODE_LATENCY_CONST_0 + TOKENIZE_LATENCY_CONST * num_items_to_decode\n",
        "\n",
        "def calculate_prefill_delay(token_count, num_items_to_prefill, TOKENIZE_LATENCY_CONST, PREFILL_LATENCY_CONST_2=0.01, PREFILL_LATENCY_CONST_1 = 0.01, PREFILL_LATENCY_CONST_0=1, PREFILL_LATENCY_CONST_MIN=0):\n",
        "    \"\"\"Calculate delay based on the token count and latency constants.\"\"\"\n",
        "    return max(PREFILL_LATENCY_CONST_MIN, (token_count * token_count * PREFILL_LATENCY_CONST_2 + token_count*PREFILL_LATENCY_CONST_1 + PREFILL_LATENCY_CONST_0 + num_items_to_prefill * TOKENIZE_LATENCY_CONST))\n",
        "\n",
        "def prefill_or_decode(env, llmactor, req_dict_prefill, req_dict, logging = False):\n",
        "    \"\"\"Main process for managing prefill, decode, or recompute operations.\"\"\"\n",
        "    while True:\n",
        "\n",
        "        with llmactor.actor.request() as req:\n",
        "\n",
        "            yield req\n",
        "            if (llmactor.get_decode_queue_size() == 0) and (llmactor.get_prefill_queue_size() == 0) and (llmactor.get_recompute_queue_size() == 0):\n",
        "                yield env.timeout(1/1000.0)\n",
        "            elif should_process_prefill_or_recompute(llmactor, env):\n",
        "                items_to_prefill = yield from fetch_prefill_items(llmactor, env)\n",
        "                prefill_delay =  process_prefill_items( llmactor, env,items_to_prefill, req_dict_prefill, req_dict)\n",
        "                if logging:\n",
        "                  print(f'llmactor {llmactor.id} Processed prefill for sequences {[x.id for x in items_to_prefill]} with delay {prefill_delay}')\n",
        "                yield env.timeout(prefill_delay)  # Assume prefill_delay is calculated somewhere\n",
        "            else:\n",
        "              if should_recompute(llmactor, env):\n",
        "                yield from remove_from_decode_store(llmactor, env, req_dict_prefill, req_dict)\n",
        "              if llmactor.get_decode_queue_size() > 0:\n",
        "                decode_delay =   yield from decode_items(llmactor, env, req_dict_prefill, req_dict)\n",
        "                yield env.timeout(decode_delay)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSZgt9TNa9rU"
      },
      "source": [
        "# LoadBalancer Class for LLM Inference\n",
        "\n",
        "This code defines a `LoadBalancer` class designed to manage and distribute inference requests across multiple Language Model (LLM) actors in a simulated environment. The class is part of a larger system for load balancing and processing requests for LLM inference.\n",
        "\n",
        "## Key Features\n",
        "\n",
        "1. **Dynamic Load Balancing**: The class can distribute requests across multiple LLM actors using various routing strategies.\n",
        "\n",
        "2. **Latency Estimation**: It can estimate prefill, decode, and total latency for requests based on historical data.\n",
        "\n",
        "3. **Resource Management**: The class tracks and manages resources like GPU memory usage and pending tokens across all LLM actors.\n",
        "\n",
        "4. **LoRA (Low-Rank Adaptation) Support**: It can handle requests that require specific LoRA configurations.\n",
        "\n",
        "5. **Queueing Mechanism**: Implements a queueing system for handling requests when the system is under high load.\n",
        "\n",
        "6. **Metrics Collection**: Provides functionality for collecting and processing metrics from all LLM actors.\n",
        "\n",
        "## Main Components\n",
        "\n",
        "- **Initialization**: Sets up the simulation environment, LLM actors, and various data structures for request management.\n",
        "\n",
        "- **Latency Estimation**: Methods like `estimate_latency` calculate expected processing times for requests.\n",
        "\n",
        "- **Pod Selection**: Functions like `find_target_pod` determine the best LLM actor to handle a given request based on various criteria.\n",
        "\n",
        "- **Request Generation and Processing**: The `process` method manages the generation of inference requests and their distribution to LLM actors.\n",
        "\n",
        "- **Queue Management**: Methods for enqueueing and dequeueing requests based on system load and request priorities.\n",
        "\n",
        "- **Resource Monitoring**: Functions to check system saturation, pending token percentages, and overall system load.\n",
        "\n",
        "## Usage\n",
        "\n",
        "The `LoadBalancer` class is designed to be used in a simulated environment, likely for testing and optimizing load balancing strategies for LLM inference systems. It would typically be instantiated with a set of LLM actors and then used to process a stream of incoming inference requests.\n",
        "\n",
        "\n",
        "This class provides a foundation for simulating and analyzing different load balancing strategies for LLM inference systems, allowing for optimization of resource utilization and response times in multi-actor setups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "MvNzDK0wZxp3"
      },
      "outputs": [],
      "source": [
        "from re import I\n",
        "from threading import active_count\n",
        "import simpy\n",
        "import numpy as np\n",
        "\n",
        "class LoadBalancer(object):\n",
        "    \"\"\"\n",
        "    LoadBalancer class to manage the distribution of requests across multiple servers.\n",
        "    It handles request queuing, target pod selection, and latency estimation.\n",
        "    \"\"\"\n",
        "    def __init__(self, env, number_of_servers = 1, list_of_llmactors = [], req_dict_prefill = {}, req_dict = {}, messages_remaining_cnt = None,  queueing_perc = 0.2):\n",
        "        \"\"\"\n",
        "        Initializes the LoadBalancer class with environment, server configurations, and other parameters.\n",
        "\n",
        "        :param env: The simulation environment.\n",
        "        :param number_of_servers: The number of servers in the load balancer.\n",
        "        :param list_of_llmactors: List of actors (servers) handling requests.\n",
        "        :param req_dict_prefill: Dictionary to store prefill request data.\n",
        "        :param req_dict: Dictionary to store request data.\n",
        "        :param messages_remaining_cnt: Counter for the number of remaining messages.\n",
        "        :param queueing_perc: Threshold percentage for queueing requests.\n",
        "        \"\"\"\n",
        "        self.number_of_servers = number_of_servers\n",
        "        assert len(list_of_llmactors) == number_of_servers\n",
        "        self.list_of_llmactors = list_of_llmactors\n",
        "        self.env = env\n",
        "        self.req_dict_prefill = req_dict_prefill\n",
        "        self.req_dict = req_dict\n",
        "        self.queues = {}\n",
        "        self.all_target_latencies = set()\n",
        "\n",
        "        self.messages_remaining_cnt = messages_remaining_cnt\n",
        "\n",
        "        self.queueing_perc = queueing_perc\n",
        "\n",
        "\n",
        "    def estimate_total_latency(self, llmactor, input_size, output_size, percentile = 95):\n",
        "        \"\"\"\n",
        "        Estimates total latency for the request processing by calculating prefill, decode, and queue times.\n",
        "\n",
        "        \"\"\"\n",
        "        estimate_total_latency_list = []\n",
        "        estimated_prefill_latency_list = []\n",
        "        estimated_decode_latency_list = []\n",
        "        estimated_queue_time_list = []\n",
        "        current_tokens_in_kv_cache = llmactor.get_num_tokens_in_decode()\n",
        "        for item in llmactor.decode_store.items:\n",
        "          tokens_in_kv_cache_at_start_of_decode = item.tokens_in_kv_cache_at_start_of_decode if  item.tokens_in_kv_cache_at_start_of_decode is not None else 0\n",
        "          decode_delays_per_output_token_normalized_by_batch_size = 0 if tokens_in_kv_cache_at_start_of_decode == 0 else ((item.end_decode_time - item.end_prefill_time)/tokens_in_kv_cache_at_start_of_decode)/(item.output_size - item.output_size_remaining )\n",
        "          estimated_decode_time = decode_delays_per_output_token_normalized_by_batch_size * current_tokens_in_kv_cache * output_size\n",
        "\n",
        "\n",
        "          estimated_queue_time = item.start_prefill_time - item.arrival_time\n",
        "\n",
        "\n",
        "          estimated_prefill_time = (item.end_prefill_time - item.arrival_time + 0.0)/item.input_size *input_size\n",
        "\n",
        "\n",
        "          estimate_total_latency_list.append(estimated_prefill_time + estimated_decode_time + estimated_queue_time)\n",
        "\n",
        "          estimated_prefill_latency_list.append(estimated_prefill_time)\n",
        "          estimated_decode_latency_list.append(estimated_decode_time)\n",
        "          estimated_queue_time_list.append(estimated_queue_time)\n",
        "\n",
        "        estimated_prefill_latency = 0 if len(estimated_prefill_latency_list) == 0 else np.percentile(estimated_prefill_latency_list, percentile)\n",
        "        estimated_decode_latency = 0 if len(estimated_decode_latency_list) == 0 else np.percentile(estimated_decode_latency_list, percentile)\n",
        "        estimated_queue_time =  0 if len(estimated_queue_time_list) == 0 else np.percentile(estimated_queue_time_list, percentile)\n",
        "        estimated_total_latency = 0 if len(estimate_total_latency_list) == 0 else np.percentile(estimate_total_latency_list, percentile)\n",
        "\n",
        "        return estimated_total_latency, estimated_prefill_latency, estimated_decode_latency\n",
        "\n",
        "    def estimate_avg_latency(self, llmactor, input_size, output_size):\n",
        "        \"\"\"\n",
        "        Estimates average latency by calculating the mean prefill, decode, and queue times.\n",
        "\n",
        "        \"\"\"\n",
        "        estimate_total_latency_list = []\n",
        "        estimated_prefill_latency_list = []\n",
        "        estimated_decode_latency_list = []\n",
        "        estimated_queue_time_list = []\n",
        "        current_tokens_in_kv_cache = llmactor.get_num_tokens_in_decode()\n",
        "        for item in llmactor.decoded_store.items:\n",
        "          tokens_in_kv_cache_at_start_of_decode = item.tokens_in_kv_cache_at_start_of_decode if  item.tokens_in_kv_cache_at_start_of_decode is not None else 0\n",
        "          decode_delays_per_output_token_normalized_by_batch_size = 0 if tokens_in_kv_cache_at_start_of_decode == 0 else ((item.end_decode_time - item.end_prefill_time)/tokens_in_kv_cache_at_start_of_decode)/(item.output_size - item.output_size_remaining )\n",
        "          estimated_decode_time = decode_delays_per_output_token_normalized_by_batch_size * current_tokens_in_kv_cache * output_size\n",
        "\n",
        "\n",
        "          estimated_queue_time = item.start_prefill_time - item.arrival_time\n",
        "\n",
        "\n",
        "          estimated_prefill_time = (item.end_prefill_time - item.arrival_time + 0.0)/item.input_size *input_size\n",
        "\n",
        "\n",
        "          estimate_total_latency_list.append(estimated_prefill_time + estimated_decode_time + estimated_queue_time)\n",
        "\n",
        "          estimated_prefill_latency_list.append(estimated_prefill_time)\n",
        "          estimated_decode_latency_list.append(estimated_decode_time)\n",
        "          estimated_queue_time_list.append(estimated_queue_time)\n",
        "\n",
        "        estimated_prefill_latency = 0 if len(estimated_prefill_latency_list) == 0 else np.mean(estimated_prefill_latency_list)\n",
        "        estimated_decode_latency = 0 if len(estimated_decode_latency_list) == 0 else np.mean(estimated_decode_latency_list)\n",
        "        estimated_queue_time =  0 if len(estimated_queue_time_list) == 0 else np.mean(estimated_queue_time_list)\n",
        "        estimated_total_latency = 0 if len(estimate_total_latency_list) == 0 else np.mean(estimate_total_latency_list)\n",
        "\n",
        "        return estimated_total_latency, estimated_prefill_latency, estimated_decode_latency\n",
        "\n",
        "\n",
        "    def check_saturations(self, max = MAX_GPU_MEMORY_PERC_BEFORE_RECOMPUTE_NON_CRITICAL):\n",
        "      \"\"\"\n",
        "        Checks if all pods are saturated based on the number of tokens in cache.\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "      all_pod_saturated = True\n",
        "      for llmactor in self.list_of_llmactors:\n",
        "        if llmactor.get_min_expected_num_tokens_in_kvcache_after_prefill()/ (llmactor.max_num_tokens_allowed + 0.0) < max:\n",
        "          all_pod_saturated = False\n",
        "      return all_pod_saturated\n",
        "\n",
        "    def get_pending_tokens_perc(self, llmactor):\n",
        "        \"\"\"\n",
        "        Calculates the percentage of pending tokens for a specific LLM actor.\n",
        "\n",
        "        \"\"\"\n",
        "        pending_tokens = 0\n",
        "        max_num_tokens_allowed = llmactor.max_num_tokens_allowed\n",
        "        for item in llmactor.decode_store.items:\n",
        "            pending_tokens += item.output_size + item.input_size\n",
        "\n",
        "        for item in llmactor.prefill_store.items:\n",
        "            pending_tokens += item.output_size + item.input_size\n",
        "        return pending_tokens/(max_num_tokens_allowed + 0.0)\n",
        "\n",
        "    def get_overall_pending_tokens_perc(self):\n",
        "        \"\"\"\n",
        "        Calculates the overall percentage of pending tokens across all LLM actors.\n",
        "\n",
        "        \"\"\"\n",
        "        total_pending_tokens = 0\n",
        "        total_max_tokens = 0\n",
        "        for llmactor in self.list_of_llmactors:\n",
        "\n",
        "          total_pending_tokens += self.get_pending_tokens_perc(llmactor) * llmactor.max_num_tokens_allowed\n",
        "          total_max_tokens += llmactor.max_num_tokens_allowed\n",
        "        return total_pending_tokens/total_max_tokens\n",
        "\n",
        "    def get_lora_affinity(self, lora_requested):\n",
        "         \"\"\"\n",
        "         Determines which pods have the requested LoRA models loaded.\n",
        "         \"\"\"\n",
        "         min_lora_count = np.inf\n",
        "         pods_with_lora_requested = []\n",
        "         if lora_requested != \"\":\n",
        "            for llmactor in self.list_of_llmactors:\n",
        "              if lora_requested in llmactor.lora_loaded:\n",
        "                pods_with_lora_requested.append(llmactor)\n",
        "         if len(pods_with_lora_requested) == 0:\n",
        "            for llmactor in self.list_of_llmactors:\n",
        "              if len(llmactor.lora_loaded) < min_lora_count:\n",
        "                min_lora_count = len(llmactor.lora_loaded)\n",
        "                pods_with_lora_requested = [llmactor]\n",
        "              elif len(llmactor.lora_loaded) == min_lora_count:\n",
        "                pods_with_lora_requested.append(llmactor)\n",
        "         return pods_with_lora_requested\n",
        "\n",
        "\n",
        "    def find_target_pod_based_on_min_latency(self, pods, input_size, output_size,  target_latency = np.inf, output_error = 0, max_tokens = 1024, buffer = 1):\n",
        "\n",
        "        \"\"\"\n",
        "        Finds the target pod with the minimum latency for processing a request.\n",
        "        \"\"\"\n",
        "        all_candiated_pods_based_on_min_expected_latency = []\n",
        "        min_expected_latency = np.inf\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        for i, llmactor in enumerate(pods):\n",
        "\n",
        "            #estimate latencies\n",
        "            output_size_for_estimation = min(max(1.0, np.round(np.abs(np.random.normal(output_size, output_size * output_error, 1))).astype(int)[0]), max_tokens)\n",
        "            estimated_latency, estimated_prefill_latency, estimated_decode_latency = self.estimate_avg_latency(llmactor, input_size, output_size_for_estimation)\n",
        "            estimated_latency_per_output_token = (estimated_latency) / output_size_for_estimation\n",
        "\n",
        "\n",
        "            if estimated_latency_per_output_token < min_expected_latency:\n",
        "              min_expected_latency = estimated_latency_per_output_token\n",
        "              all_candiated_pods_based_on_min_expected_latency = [i]\n",
        "            elif estimated_latency_per_output_token == min_expected_latency:\n",
        "              all_candiated_pods_based_on_min_expected_latency.append(i)\n",
        "\n",
        "\n",
        "        # return a random one\n",
        "        if len(all_candiated_pods_based_on_min_expected_latency) > 0:\n",
        "          random_pod_index = random.choice(all_candiated_pods_based_on_min_expected_latency)\n",
        "          return self.list_of_llmactors[random_pod_index], estimated_latency_per_output_token\n",
        "        else:\n",
        "          return None, estimated_latency_per_output_token\n",
        "\n",
        "\n",
        "    def find_target_pod_based_on_max_pending(self, pods, input_size, output_size,  target_latency = np.inf, output_error = 0, max_tokens = 1024, buffer = 0.75):\n",
        "\n",
        "        \"\"\"\n",
        "        Finds the target pod based on maximum pending requests under a certain latency.\n",
        "        \"\"\"\n",
        "        all_candiated_pods_based_on_maximum_pending_req = []\n",
        "\n",
        "        min_pending_token_perc = np.inf\n",
        "        max_pending_token_below_target_perc = 0\n",
        "        min_kv_cache_usage = np.inf\n",
        "        drop_request = False\n",
        "        if target_latency == np.inf:\n",
        "          drop_request = True\n",
        "        max_tokens_in_kv_before_eviction = MAX_GPU_MEMORY_PERC_BEFORE_RECOMPUTE\n",
        "        latency_estimations = {}\n",
        "\n",
        "        min_lora_count = np.inf\n",
        "\n",
        "\n",
        "        for i, llmactor in enumerate(pods):\n",
        "\n",
        "            #estimate latencies\n",
        "            output_size_for_estimation = min(max(1.0, np.round(np.abs(np.random.normal(output_size, output_size * output_error, 1))).astype(int)[0]), max_tokens)\n",
        "            estimated_latency, estimated_prefill_latency, estimated_decode_latency = self.estimate_total_latency(llmactor, input_size, output_size_for_estimation)\n",
        "            estimated_latency_per_output_token = (estimated_latency) / output_size_for_estimation\n",
        "            latency_estimations[i] = estimated_latency_per_output_token\n",
        "\n",
        "            #get pending tokens\n",
        "            pending_token_perc = self.get_pending_tokens_perc(llmactor)\n",
        "\n",
        "\n",
        "            # get matching pods\n",
        "            if estimated_latency_per_output_token < buffer*target_latency and pending_token_perc > max_pending_token_below_target_perc:\n",
        "              max_pending_token_below_target_perc = pending_token_perc\n",
        "              all_candiated_pods_based_on_maximum_pending_req = [i]\n",
        "            elif estimated_latency_per_output_token <  buffer*target_latency and pending_token_perc == max_pending_token_below_target_perc:\n",
        "              all_candiated_pods_based_on_maximum_pending_req.append(i)\n",
        "\n",
        "\n",
        "        # return a random one\n",
        "        if len(all_candiated_pods_based_on_maximum_pending_req) > 0:\n",
        "          random_pod_index = random.choice(all_candiated_pods_based_on_maximum_pending_req)\n",
        "          return self.list_of_llmactors[random_pod_index],   latency_estimations[random_pod_index]\n",
        "        else:\n",
        "          return None, 0\n",
        "\n",
        "\n",
        "    def find_target_pod_based_on_min_pending(self, pods, eviction_safe = False, max_kv_perc = MAX_GPU_MEMORY_PERC_BEFORE_RECOMPUTE ):\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        Finds the target pod based on the minimum pending tokens and safe eviction condition.\n",
        "        \"\"\"\n",
        "        all_candiated_pods_based_on_minimum_pending_req = []\n",
        "        min_pending_token_perc = np.inf\n",
        "\n",
        "        for i, llmactor in enumerate(pods):\n",
        "            pending_token_perc = self.get_pending_tokens_perc(llmactor)\n",
        "            expected_kvcache_usage_after_prefill = llmactor.get_min_expected_num_tokens_in_kvcache_after_prefill() / (llmactor.max_num_tokens_allowed + 0.0)\n",
        "\n",
        "            if eviction_safe:\n",
        "              if pending_token_perc < min_pending_token_perc and expected_kvcache_usage_after_prefill < max_kv_perc:\n",
        "                all_candiated_pods_based_on_minimum_pending_req = [i]\n",
        "                min_pending_token_perc = pending_token_perc\n",
        "              elif pending_token_perc == min_pending_token_perc:\n",
        "                all_candiated_pods_based_on_minimum_pending_req.append(i)\n",
        "            else:\n",
        "               if pending_token_perc < min_pending_token_perc:\n",
        "                all_candiated_pods_based_on_minimum_pending_req = [i]\n",
        "                min_pending_token_perc = pending_token_perc\n",
        "               elif pending_token_perc == min_pending_token_perc:\n",
        "                all_candiated_pods_based_on_minimum_pending_req.append(i)\n",
        "\n",
        "\n",
        "        # return a random one\n",
        "        if len(all_candiated_pods_based_on_minimum_pending_req) > 0:\n",
        "          random_pod_index = random.choice(all_candiated_pods_based_on_minimum_pending_req)\n",
        "          return self.list_of_llmactors[random_pod_index]\n",
        "        else:\n",
        "          return None\n",
        "\n",
        "\n",
        "    def find_target_pod_based_on_min_kv_cache(self, pods, ):\n",
        "\n",
        "        \"\"\"\n",
        "        Finds the target pod with the minimum usage of KV cache.\n",
        "        \"\"\"\n",
        "\n",
        "        all_candiated_pods_based_on_min_kv_cache_usage = []\n",
        "        min_kv_cache_usage = np.inf\n",
        "\n",
        "        for i, llmactor in enumerate(pods):\n",
        "            expected_kvcache_usage_after_prefill = llmactor.get_min_expected_num_tokens_in_kvcache_after_prefill() / (llmactor.MAX_NUM_TOKENS_ALLOWED + 0.0)\n",
        "\n",
        "            if expected_kvcache_usage_after_prefill < min_kv_cache_usage:\n",
        "              min_kv_cache_usage = expected_kvcache_usage_after_prefill\n",
        "              all_candiated_pods_based_on_min_kv_cache_usage = [i]\n",
        "            elif expected_kvcache_usage_after_prefill == min_kv_cache_usage:\n",
        "              all_candiated_pods_based_on_min_kv_cache_usage.append(i)\n",
        "\n",
        "\n",
        "        # return a random one\n",
        "        if len(all_candiated_pods_based_on_min_kv_cache_usage) > 0:\n",
        "          random_pod_index = random.choice(all_candiated_pods_based_on_min_kv_cache_usage)\n",
        "          return self.list_of_llmactors[random_pod_index]\n",
        "        else:\n",
        "          return None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def find_target_pod(self, routing_type, input_size, output_size,  target_latency = np.inf, lora_requested = \"\", output_error = 0, max_tokens = 1024):\n",
        "        \"\"\"\n",
        "        Finds the target pod based on routing strategy and various factors like latency, pending tokens, or LoRA.\n",
        "        \"\"\"\n",
        "\n",
        "        target_pod = None\n",
        "        latency_esimated = 0\n",
        "\n",
        "        active_req_target_latency_in_window = self.getActiveReqTargetLatencyInWindow()\n",
        "        violations_present = self.getViolationsTargetLatencyInWindow()\n",
        "\n",
        "\n",
        "        if target_latency == np.inf:\n",
        "          all_pods_saturated = self.check_saturations()\n",
        "          if all_pods_saturated and len(active_req_target_latency_in_window) > 0:\n",
        "            return target_pod, latency_esimated\n",
        "          if violations_present and len(active_req_target_latency_in_window) > 0:\n",
        "            return target_pod, latency_esimated\n",
        "\n",
        "        if routing_type == \"least\":\n",
        "          target_pod = self.find_target_pod_based_on_min_kv_cache(self.list_of_llmactors, )\n",
        "          return target_pod, latency_esimated\n",
        "        if routing_type == \"leastPseudo\":\n",
        "          target_pod = self.find_target_pod_based_on_min_pending(self.list_of_llmactors, eviction_safe=False)\n",
        "          return target_pod, latency_esimated\n",
        "        elif routing_type == \"leastlatency\":\n",
        "          target_pod, latency_esimated = self.find_target_pod_based_on_min_latency(self.list_of_llmactors, input_size, output_size, target_latency)\n",
        "          return target_pod, latency_esimated\n",
        "        elif lora_requested != \"\":\n",
        "          lora_pods = self.get_lora_affinity(lora_requested)\n",
        "          target_pod, latency_esimated = self.find_target_pod_based_on_max_pending(lora_pods, input_size, output_size, target_latency)\n",
        "          if target_pod is  None:\n",
        "            target_pod, latency_esimated = self.find_target_pod_based_on_max_pending(self.list_of_llmactors, input_size, output_size, target_latency)\n",
        "          if target_pod is None:\n",
        "            target_pod = self.find_target_pod_based_on_min_pending(lora_pods, eviction_safe=True)\n",
        "          if target_pod is None:\n",
        "            target_pod = self.find_target_pod_based_on_min_pending(self.list_of_llmactors,  eviction_safe=False)\n",
        "          return target_pod, latency_esimated\n",
        "\n",
        "        else:\n",
        "          pods = self.list_of_llmactors\n",
        "          target_pod, latency_esimated = self.find_target_pod_based_on_max_pending(pods, input_size, output_size, target_latency)\n",
        "          if target_pod is None:\n",
        "            target_pod = self.find_target_pod_based_on_min_pending(self.list_of_llmactors,  eviction_safe=False)\n",
        "          return target_pod, latency_esimated\n",
        "\n",
        "\n",
        "    def queueing_signal(self):\n",
        "      \"\"\"\n",
        "        Checks if all pods are saturated based on the queueing percentage.\n",
        "      \"\"\"\n",
        "      all_pod_saturated = self.get_overall_pending_tokens_perc() > self.queueing_perc\n",
        "\n",
        "\n",
        "      return all_pod_saturated\n",
        "\n",
        "\n",
        "    def dequeueing_signal(self):\n",
        "      \"\"\"\n",
        "        Checks if the system should start dequeueing requests from the queue.\n",
        "      \"\"\"\n",
        "      all_pod_saturated = self.get_overall_pending_tokens_perc() > self.queueing_perc\n",
        "\n",
        "\n",
        "      return all_pod_saturated\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def check_if_queues_empty(self):\n",
        "        \"\"\"\n",
        "        Checks if all queues are empty.\n",
        "        \"\"\"\n",
        "        for k, v in self.queues.items():\n",
        "          if not v.empty():\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def dequeue(self):\n",
        "        \"\"\"\n",
        "        Dequeues the highest-priority request from the queue. Current logic is simple, deque in order of active target latenices\n",
        "        \"\"\"\n",
        "        active_targets = sorted(self.getActiveReqTargetLatencyInWindow(np.inf))\n",
        "        for k in active_targets:\n",
        "          if k in self.queues and not self.queues[k].empty():\n",
        "            req = self.queues[k].get()\n",
        "            return req\n",
        "        return None\n",
        "\n",
        "\n",
        "    def dequeue_process(self, routing_type, drop_late_requests = False):\n",
        "        \"\"\"\n",
        "        Continuously dequeues requests and routes them to target pods for processing.\n",
        "\n",
        "        :param routing_type: The routing strategy to use for dequeuing requests.\n",
        "        :param drop_late_requests: Flag to drop requests that exceed target latency.\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            if not self.check_if_queues_empty() and not self.dequeueing_signal():\n",
        "                # Get the request with the highest SLO violation\n",
        "                req = self.dequeue()\n",
        "                if   req:\n",
        "                  #if self.env.now - req.arrival_time > req.target_latency:\n",
        "                      #print(f\"dropped req {req.id} {req.target_latency}\")\n",
        "                  if (drop_late_requests == False) or (self.env.now - req.arrival_time < 100*req.target_latency): #ad-hoc\n",
        "                    target_pod, estimated_latency = self.find_target_pod(routing_type, req.input_size, req.output_size, req.target_latency, req.lora)\n",
        "                    req.target_pod = target_pod.id\n",
        "                    req.estimated_latency = estimated_latency\n",
        "                    req.queue_size_before_prefill = target_pod.get_prefill_queue_size()\n",
        "\n",
        "                    #print(f\"dequeued req {req.id} {req.target_latency}\")\n",
        "                    # Send it to the appropriate pod for processing\n",
        "                    target_pod = self.list_of_llmactors[req.target_pod]\n",
        "                    target_pod.prefill_store.put(req)\n",
        "                    self.req_dict_prefill[req.id] = req\n",
        "                    #print(f\"Dequeued request {req.id} and sent to pod {req.target_pod}\")\n",
        "                else:\n",
        "                    yield self.env.timeout(0.001)  # Adjust the delay as per your requirements\n",
        "            # Check again after a small delay\n",
        "            else:\n",
        "              yield self.env.timeout(0.001)  # Adjust the delay as per your requirements\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def getActiveReqTargetLatencyInWindow(self, time_windows = 300):\n",
        "      \"\"\"\n",
        "        Retrieves active target latencies within a specified time window.\n",
        "\n",
        "        :param time_windows: Time window in which to check for active requests.\n",
        "        :return: Set of active target latencies.\n",
        "      \"\"\"\n",
        "      active_targets = set()\n",
        "      for llmactor in self.list_of_llmactors:\n",
        "        if llmactor.get_prefill_queue_size() > 0:\n",
        "          for req in llmactor.prefill_store.items:\n",
        "            if req.target_latency != np.inf:\n",
        "              active_targets.add(req.target_latency)\n",
        "        if llmactor.get_decode_queue_size() > 0:\n",
        "          for req in llmactor.decode_store.items:\n",
        "            if req.target_latency != np.inf:\n",
        "              active_targets.add(req.target_latency)\n",
        "        if llmactor.get_recompute_queue_size() > 0:\n",
        "          for req in llmactor.recompute_store.items:\n",
        "            if req.item.target_latency != np.inf:\n",
        "              active_targets.add(req.item.target_latency)\n",
        "        if llmactor.get_decoded_queue_size() > 0:\n",
        "          for req in llmactor.decoded_store.items:\n",
        "            if (req.target_latency != np.inf) and (self.env.now - req.arrival_time < time_windows) :\n",
        "              active_targets.add(req.target_latency)\n",
        "      return active_targets\n",
        "\n",
        "    def getViolationsTargetLatencyInWindow(self, time_windows = 300, percentile = 0.04):\n",
        "      \"\"\"\n",
        "        Checks for latency violations within a specified time window.\n",
        "\n",
        "        :param time_windows: Time window in which to check for latency violations.\n",
        "        :param percentile: The violation threshold percentile.\n",
        "        :return: Boolean indicating if violations occurred.\n",
        "      \"\"\"\n",
        "      didViolate = False\n",
        "      violation_dict = {}\n",
        "      req_dict = {}\n",
        "      for llmactor in self.list_of_llmactors:\n",
        "        if llmactor.get_decoded_queue_size() > 0:\n",
        "          for req in llmactor.decoded_store.items :\n",
        "              if  (req.target_latency == np.inf)  or  (self.env.now - req.arrival_time > time_windows):\n",
        "                continue\n",
        "              if req.target_latency not in violation_dict:\n",
        "                violation_dict[req.target_latency] = 0\n",
        "              if req.target_latency not in req_dict:\n",
        "                req_dict[req.target_latency] = 0\n",
        "\n",
        "              req_dict[req.target_latency] += 1\n",
        "              if ((req.end_decode_time - req.arrival_time)/req.output_size > req.target_latency ):\n",
        "                  violation_dict[req.target_latency] += 1\n",
        "\n",
        "\n",
        "      for target_latency in violation_dict:\n",
        "        if violation_dict[target_latency]/req_dict[target_latency] > percentile:\n",
        "          didViolate = True\n",
        "      return didViolate\n",
        "\n",
        "\n",
        "\n",
        "    def generate_request_inference_gateway( self, rate, lora_requested,  target_latency_list, prefix_latency_list,  routing_type = \"random\",  prompt_output_tuple = None, mean_request_size = None, std_request_size = None, mean_output_size = None, std_output_size = None, estimated_output_size = None):\n",
        "        \"\"\"\n",
        "        Generates and routes requests through the inference gateway based on the provided parameters.\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        cnt = 0\n",
        "        timeout_interval = 1 / rate\n",
        "\n",
        "\n",
        "        while True :\n",
        "          #print(self.messages_remaining_cnt)\n",
        "          if self.messages_remaining_cnt is not None and  self.messages_remaining_cnt <= 0:\n",
        "            break\n",
        "\n",
        "          target_latency_index = random.choice([x for x in range(len(target_latency_list))]) #random.choice([0.02, 0.1])\n",
        "          target_latency = target_latency_list[target_latency_index]\n",
        "          prefix = prefix_latency_list[target_latency_index]\n",
        "          if prompt_output_tuple is None:\n",
        "            mean_request_size = mean_request_size\n",
        "            std_request_size = std_request_size\n",
        "            mean_output_size = mean_output_size\n",
        "            std_output_size = std_output_size\n",
        "            input_size = determine_size(mean_request_size, std_request_size, None, None)\n",
        "            output_size = determine_size(mean_output_size, std_output_size, None, None)\n",
        "          else:\n",
        "            input_output_size = prompt_output_tuple[random.randint(0, len(prompt_output_tuple)-1)]\n",
        "            input_size = input_output_size[0]\n",
        "            output_size = input_output_size[1]\n",
        "\n",
        "          request_id = str(f\"{prefix}: {cnt}\")\n",
        "\n",
        "          new_req = create_request(request_id, self.env.now, input_size, output_size)\n",
        "          new_req.target_latency = target_latency\n",
        "          cnt += 1\n",
        "          self.messages_remaining_cnt -= 1\n",
        "          if routing_type == \"random\":\n",
        "            target_pod = self.list_of_llmactors[random.randint(0, self.number_of_servers-1)]\n",
        "            new_req.target_pod = target_pod.id\n",
        "\n",
        "            new_req.queue_size_before_prefill = target_pod.get_prefill_queue_size()\n",
        "            if lora_requested != \"\":\n",
        "                  new_req.lora = lora_requested\n",
        "\n",
        "            target_pod.prefill_store.put(new_req)\n",
        "            self.req_dict_prefill[new_req.id] = new_req\n",
        "\n",
        "            yield self.env.timeout(timeout_interval)\n",
        "          elif  (self.queueing_signal()) or (self.check_if_queues_empty() is False):\n",
        "                  #print(f\"queued req {new_req.id} {target_latency}\")\n",
        "                  if lora_requested != \"\":\n",
        "                    new_req.lora = lora_requested\n",
        "                  target_queue = self.queues.get(target_latency, queue.Queue())\n",
        "                  target_queue.put(new_req)\n",
        "                  self.queues[target_latency] = target_queue\n",
        "\n",
        "                  yield self.env.timeout(timeout_interval)\n",
        "          elif routing_type == \"least\":\n",
        "            if estimated_output_size is None:\n",
        "                  estimated_output_size = output_size\n",
        "\n",
        "            target_pod, estimated_latency = self.find_target_pod(routing_type, input_size, estimated_output_size, target_latency, lora_requested)\n",
        "            if target_pod is not None:\n",
        "              new_req.target_pod = target_pod.id\n",
        "              new_req.estimated_latency = estimated_latency\n",
        "              new_req.queue_size_before_prefill = target_pod.get_prefill_queue_size()\n",
        "              if lora_requested != \"\":\n",
        "                  new_req.lora = lora_requested\n",
        "\n",
        "              target_pod.prefill_store.put(new_req)\n",
        "              self.req_dict_prefill[new_req.id] = new_req\n",
        "\n",
        "\n",
        "            yield self.env.timeout(timeout_interval)\n",
        "\n",
        "          elif routing_type == \"leastlatency\":\n",
        "            if estimated_output_size is None:\n",
        "                  estimated_output_size = output_size\n",
        "\n",
        "            target_pod, estimated_latency = self.find_target_pod(routing_type, input_size, estimated_output_size, target_latency, lora_requested)\n",
        "\n",
        "            if target_pod is not None:\n",
        "              new_req.target_pod = target_pod.id\n",
        "              new_req.estimated_latency = estimated_latency\n",
        "              new_req.queue_size_before_prefill = target_pod.get_prefill_queue_size()\n",
        "              if lora_requested != \"\":\n",
        "                  new_req.lora = lora_requested\n",
        "\n",
        "              target_pod.prefill_store.put(new_req)\n",
        "              self.req_dict_prefill[new_req.id] = new_req\n",
        "\n",
        "\n",
        "            yield self.env.timeout(timeout_interval)\n",
        "\n",
        "          elif routing_type == \"leastPseudo\":\n",
        "            if estimated_output_size is None:\n",
        "                  estimated_output_size = output_size\n",
        "\n",
        "            target_pod, estimated_latency = self.find_target_pod(routing_type, input_size, estimated_output_size, target_latency, lora_requested)\n",
        "\n",
        "            if target_pod is not None:\n",
        "              new_req.target_pod = target_pod.id\n",
        "              new_req.estimated_latency = estimated_latency\n",
        "              new_req.queue_size_before_prefill = target_pod.get_prefill_queue_size()\n",
        "              if lora_requested != \"\":\n",
        "                  new_req.lora = lora_requested\n",
        "\n",
        "              target_pod.prefill_store.put(new_req)\n",
        "              self.req_dict_prefill[new_req.id] = new_req\n",
        "\n",
        "            yield self.env.timeout(timeout_interval)\n",
        "\n",
        "          elif routing_type == \"smart\":\n",
        "\n",
        "\n",
        "                if estimated_output_size is None:\n",
        "                  estimated_output_size = output_size\n",
        "                target_pod, estimated_latency = self.find_target_pod(routing_type, input_size, estimated_output_size, target_latency, lora_requested)\n",
        "                if target_pod is not None:\n",
        "\n",
        "\n",
        "                  new_req.target_pod = target_pod.id\n",
        "                  new_req.estimated_latency = estimated_latency\n",
        "                  new_req.queue_size_before_prefill = target_pod.get_prefill_queue_size()\n",
        "\n",
        "                  target_pod.prefill_store.put(new_req)\n",
        "                  self.req_dict_prefill[new_req.id] = new_req\n",
        "\n",
        "                #else:\n",
        "                  #print(f\"No target pod found for request {request_id}\")\n",
        "\n",
        "\n",
        "                yield self.env.timeout(timeout_interval)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def process(self,rate, lora_requested, target_latency_list, prefix_latency_list, routing_type, prompt_output_tuple, mean_request_size, std_request_size, mean_output_size, std_output_size, estimated_output_size):\n",
        "           self.env.process(self.generate_request_inference_gateway( rate, lora_requested, target_latency_list, prefix_latency_list, routing_type, prompt_output_tuple,\n",
        "                                       mean_request_size = mean_request_size,\n",
        "                                       std_request_size=std_request_size,\n",
        "                                       mean_output_size = mean_output_size,\n",
        "                                       std_output_size=std_output_size, estimated_output_size = estimated_output_size))\n",
        "           self.env.process(self.dequeue_process(routing_type), )\n",
        "           for llmactor in self.list_of_llmactors:\n",
        "              self.env.process(prefill_or_decode(self.env, llmactor, self.req_dict_prefill, self.req_dict))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def metrics(self):\n",
        "        for llmactor in self.list_of_llmactors:\n",
        "              self.env.process(metrics(self.env, llmactor))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XVNJop5cFP9"
      },
      "source": [
        "# LLM Inference Simulation and Analysis\n",
        "\n",
        "This code sets up and runs a simulation of Language Model (LLM) inference requests across multiple routing strategies. It then analyzes the performance of each strategy under various load conditions.\n",
        "\n",
        "## Key Components\n",
        "\n",
        "1. **Simulation Setup**:\n",
        "   - Defines different request rates, durations, and request sizes for two types of requests: \"lo\" (low-latency) and \"hi\" (high-latency).\n",
        "   - Sets up multiple routing strategies: \"leastPseudo\", \"smart\", \"leastlatency\", \"least\", and \"random\".\n",
        "\n",
        "2. **Results Structure**:\n",
        "   - Creates a nested dictionary `results` to store performance metrics for each routing strategy.\n",
        "   - Metrics include latency, throughput, time to first token (ttft), time per output token (tpot), recompute count, and percentage of requests meeting latency targets.\n",
        "\n",
        "3. **Simulation Loop**:\n",
        "   - Iterates over each routing strategy and request rate.\n",
        "   - For each combination, it sets up a simpy environment with multiple LLM actors and a LoadBalancer.\n",
        "   - Runs the simulation for a specified duration.\n",
        "\n",
        "4. **Data Collection and Analysis**:\n",
        "   - Collects data on completed requests, filtering out the first 10% to account for warm-up effects.\n",
        "   - Calculates various performance metrics for both \"lo\" and \"hi\" request types.\n",
        "   - Tracks which pods (LLM actors) processed each request.\n",
        "\n",
        "5. **Results Storage**:\n",
        "   - Stores calculated metrics in the `results` dictionary for each routing strategy and request rate.\n",
        "\n",
        "6. **Output**:\n",
        "   - Prints summary statistics for each simulation run, including request distribution across pods, latency, and percentage of requests meeting latency targets.\n",
        "\n",
        "## Key Metrics\n",
        "\n",
        "- **Latency**: Average time from request arrival to completion, per output token.\n",
        "- **Throughput**: Number of tokens processed (both input and output) per unit time.\n",
        "- **TTFT (Time to First Token)**: Average time from request arrival to the end of the prefill phase.\n",
        "- **TPOT (Time Per Output Token)**: Average time to generate each output token.\n",
        "- **Recompute Count**: Average number of times a request had to be recomputed.\n",
        "- **Percentage Below Latency Target**: Percentage of requests completed within the specified latency target.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hbJwWTRear8c"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Define latency targets (in seconds or the appropriate unit for your system)\n",
        "\n",
        "\n",
        "# Your rates, durations, request sizes, etc.\n",
        "rates_lo = [ 30, 25, 20, 15, 10, 5, 1 ]\n",
        "rates_hi = [ 30, 25, 20, 15, 10, 5, 1 ]\n",
        "no_of_messages = 2500\n",
        "SIM_DURATIONS = [no_of_messages/r + 100 for r in rates_lo]\n",
        "mean_request_size_1 = 202\n",
        "std_request_size_1 = 20\n",
        "mean_output_size_1 =179\n",
        "std_output_size_1 = 17\n",
        "\n",
        "mean_request_size_2 = 202\n",
        "std_request_size_2 = 20\n",
        "mean_output_size_2 =179\n",
        "std_output_size_2 = 17\n",
        "\n",
        "lora_requested_lo = \"dummy-1\"\n",
        "lora_requested_hi = \"dummy-2\"\n",
        "\n",
        "# Define a structure to store results for all routing types\n",
        "results = {\n",
        "    'leastPseudo': {'latency': [], 'latency_lo': [], 'latency_hi': [],\n",
        "               'throughput_prefill': [], 'throughput_decode': [],\n",
        "               'throughput_prefill_lo': [], 'throughput_decode_lo': [],\n",
        "               'throughput_prefill_hi': [], 'throughput_decode_hi': [],\n",
        "               'ttft': [], 'ttft_lo': [], 'ttft_hi': [],\n",
        "               'tpot': [], 'tpot_lo': [], 'tpot_hi': [],\n",
        "               'target_pods_lo': [], 'target_pods_hi': [],\n",
        "               'recompute_cnt' : [], 'recompute_cnt_hi' : [], 'recompute_cnt_lo' : [],\n",
        "               'pct_below_latency_target_lo': [], 'pct_below_latency_target_hi': []},\n",
        "\n",
        "    'smart': {'latency': [], 'latency_lo': [], 'latency_hi': [],\n",
        "              'estimated_latency': [], 'estimated_latency_lo': [], 'estimated_latency_hi': [],\n",
        "              'throughput_prefill': [], 'throughput_decode': [],\n",
        "              'throughput_prefill_lo': [], 'throughput_decode_lo': [],\n",
        "              'throughput_prefill_hi': [], 'throughput_decode_hi': [],\n",
        "              'ttft': [], 'ttft_lo': [], 'ttft_hi': [],\n",
        "              'tpot': [], 'tpot_lo': [], 'tpot_hi': [],\n",
        "              'target_pods_lo': [], 'target_pods_hi': [],\n",
        "              'recompute_cnt' : [], 'recompute_cnt_hi' : [], 'recompute_cnt_lo' : [],\n",
        "              'pct_below_latency_target_lo': [], 'pct_below_latency_target_hi': []},\n",
        "\n",
        "    'leastlatency': {'latency': [], 'latency_lo': [], 'latency_hi': [],\n",
        "                'throughput_prefill': [], 'throughput_decode': [],\n",
        "                'throughput_prefill_lo': [], 'throughput_decode_lo': [],\n",
        "                'throughput_prefill_hi': [], 'throughput_decode_hi': [],\n",
        "                'ttft': [], 'ttft_lo': [], 'ttft_hi': [],\n",
        "                'tpot': [], 'tpot_lo': [], 'tpot_hi': [],\n",
        "                'target_pods_lo': [], 'target_pods_hi': [],\n",
        "                'recompute_cnt' : [], 'recompute_cnt_hi' : [], 'recompute_cnt_lo' : [],\n",
        "                'pct_below_latency_target_lo': [], 'pct_below_latency_target_hi': []},\n",
        "\n",
        "    'least': {'latency': [], 'latency_lo': [], 'latency_hi': [],\n",
        "                'throughput_prefill': [], 'throughput_decode': [],\n",
        "                'throughput_prefill_lo': [], 'throughput_decode_lo': [],\n",
        "                'throughput_prefill_hi': [], 'throughput_decode_hi': [],\n",
        "                'ttft': [], 'ttft_lo': [], 'ttft_hi': [],\n",
        "                'tpot': [], 'tpot_lo': [], 'tpot_hi': [],\n",
        "                'target_pods_lo': [], 'target_pods_hi': [],\n",
        "                'recompute_cnt' : [], 'recompute_cnt_hi' : [], 'recompute_cnt_lo' : [],\n",
        "                'pct_below_latency_target_lo': [], 'pct_below_latency_target_hi': []},\n",
        "\n",
        "     'random': {'latency': [], 'latency_lo': [], 'latency_hi': [],\n",
        "                'throughput_prefill': [], 'throughput_decode': [],\n",
        "                'throughput_prefill_lo': [], 'throughput_decode_lo': [],\n",
        "                'throughput_prefill_hi': [], 'throughput_decode_hi': [],\n",
        "                'ttft': [], 'ttft_lo': [], 'ttft_hi': [],\n",
        "                'tpot': [], 'tpot_lo': [], 'tpot_hi': [],\n",
        "                'target_pods_lo': [], 'target_pods_hi': [],\n",
        "                'recompute_cnt' : [], 'recompute_cnt_hi' : [], 'recompute_cnt_lo' : [],\n",
        "                'pct_below_latency_target_lo': [], 'pct_below_latency_target_hi': []},\n",
        "}\n",
        "\n",
        "all_routing_types = [ \"leastPseudo\", \"leastlatency\", \"least\", \"smart\" ]\n",
        "prompt_output_tuple = None\n",
        "\n",
        "# Iterate over routing types\n",
        "for routing_type in all_routing_types:\n",
        "    print(f'Routing Type: {routing_type}')\n",
        "\n",
        "    for i, _ in enumerate(rates_lo):\n",
        "        req_dict = {}\n",
        "        req_dict_prefill = {}\n",
        "        SIM_DURATION = SIM_DURATIONS[i]\n",
        "        print(f'Simulate with rate: for lo {rates_lo[i]} and for hi {rates_hi[i]} and routing type: {routing_type}')\n",
        "\n",
        "        # Simpy environment and LLM actors setup\n",
        "        env = simpy.Environment()\n",
        "        number_of_servers =6\n",
        "        list_of_llmactors = [LLMActor(env, 1, id) for id in range(number_of_servers)]\n",
        "        lb = LoadBalancer(env, number_of_servers=number_of_servers, list_of_llmactors=list_of_llmactors, req_dict_prefill=req_dict_prefill, req_dict=req_dict, messages_remaining_cnt=no_of_messages*2)\n",
        "        if routing_type == \"smart\":\n",
        "          lb.queueing_perc = 0.6\n",
        "        elif routing_type == \"least\":\n",
        "          lb.queueing_perc = 0.20\n",
        "        elif routing_type == \"leastlatency\":\n",
        "          lb.queueing_perc = 0.20\n",
        "        elif routing_type == \"leastPseudo\":\n",
        "          lb.queueing_perc = 0.20\n",
        "        target_latency_list_lo = [0.025]\n",
        "        prefix_latency_list_lo = [\"lo\"]\n",
        "        target_latency_list_hi = [0.5]\n",
        "        prefix_latency_list_hi = [\"hi\"]\n",
        "\n",
        "        estimated_output_size = mean_output_size_1\n",
        "        lb.process(rates_lo[i], lora_requested_lo, target_latency_list_lo, prefix_latency_list_lo, routing_type, prompt_output_tuple, mean_request_size_1, std_request_size_1, mean_output_size_1, std_output_size_1, estimated_output_size)\n",
        "        lb.process(rates_hi[i], lora_requested_hi, target_latency_list_hi, prefix_latency_list_hi, routing_type, prompt_output_tuple, mean_request_size_2, std_request_size_2, mean_output_size_2, std_output_size_2, estimated_output_size)\n",
        "        env.run(until=SIM_DURATION)\n",
        "\n",
        "        # Track which pod processed each request (lo and hi)\n",
        "        target_pods_lo = []\n",
        "        target_pods_hi = []\n",
        "\n",
        "        for req in req_dict.values():\n",
        "            if \"lo:\" in req.id:\n",
        "                target_pods_lo.append(req.target_pod)\n",
        "            elif \"hi:\" in req.id:\n",
        "                target_pods_hi.append(req.target_pod)\n",
        "\n",
        "        # Completed requests\n",
        "        completed_req = list(filter(lambda x: x.output_size_remaining == 0, req_dict.values()))\n",
        "        completed_req_lo = list(filter(lambda x: x.output_size_remaining == 0 and (\"lo:\" in x.id), req_dict.values()))\n",
        "        completed_req_hi = list(filter(lambda x: x.output_size_remaining == 0 and (\"hi:\" in x.id), req_dict.values()))\n",
        "\n",
        "        completed_req_sorted = sorted(completed_req, key=lambda x: x.end_decode_time)\n",
        "        completed_req_lo_sorted = sorted(completed_req_lo, key=lambda x: x.end_decode_time)\n",
        "        completed_req_hi_sorted = sorted(completed_req_hi, key=lambda x: x.end_decode_time)\n",
        "\n",
        "        # Exclude the first 10% of requests based on end_decode_time\n",
        "        #exclude_count = int(0.1 * len(completed_req_sorted))\n",
        "        #exclude_count_lo = int(0.1 * len(completed_req_lo_sorted))\n",
        "        #exclude_count_hi = int(0.1 * len(completed_req_hi_sorted))\n",
        "\n",
        "        # Filter out the first 10%\n",
        "        filtered_req = completed_req_sorted\n",
        "        filtered_req_lo = completed_req_lo_sorted\n",
        "        filtered_req_hi = completed_req_hi_sorted\n",
        "\n",
        "\n",
        "        # Calculate ttft, tpot, latency, and throughput\n",
        "        ttft_cur = np.mean([x.end_prefill_time - x.arrival_time for x in req_dict.values()])\n",
        "        ttft_cur_lo = np.mean([x.end_decode_time - x.arrival_time for x in filtered_req_lo])\n",
        "        ttft_cur_hi = np.mean([x.end_decode_time - x.arrival_time for x in filtered_req_hi])\n",
        "\n",
        "        tpot_cur = np.mean([(x.end_decode_time - x.start_prefill_time) / (x.output_size - x.output_size_remaining) for x in req_dict.values()])\n",
        "        tpot_cur_lo = np.mean([(x.end_decode_time - x.start_prefill_time) / (x.output_size - x.output_size_remaining) for x in filtered_req_lo])\n",
        "        tpot_cur_hi = np.mean([(x.end_decode_time - x.start_prefill_time) / (x.output_size - x.output_size_remaining) for x in filtered_req_hi])\n",
        "\n",
        "        latency_cur = np.mean([(x.end_decode_time - x.arrival_time) / (x.output_size - x.output_size_remaining) for x in filtered_req])\n",
        "        latency_cur_lo = np.mean([(x.end_decode_time - x.arrival_time) / (x.output_size - x.output_size_remaining) for x in filtered_req_lo])\n",
        "        latency_cur_hi = np.mean([(x.end_decode_time - x.arrival_time) / (x.output_size - x.output_size_remaining) for x in filtered_req_hi])\n",
        "\n",
        "        estimated_latency_cur = np.mean([x.estimated_latency for x in filtered_req])\n",
        "        estimated_latency_cur_lo = np.mean([x.estimated_latency for x in filtered_req_lo])\n",
        "        estimated_latency_cur_hi = np.mean([x.estimated_latency for x in filtered_req_hi])\n",
        "\n",
        "        recompute_cur = np.sum([x.recompute_count for x in filtered_req]) / len(filtered_req)\n",
        "        recompute_cur_lo = np.sum([x.recompute_count for x in filtered_req_lo]) / len(filtered_req_lo)\n",
        "        recompute_cur_hi = np.sum([x.recompute_count for x in filtered_req_hi]) / len(filtered_req_hi)\n",
        "\n",
        "        tt = SIM_DURATION\n",
        "        throughput_prefill_cur = np.sum([x.input_size for x in filtered_req]) / tt\n",
        "        throughput_decode_cur = np.sum([max(0, x.output_size - x.output_size_remaining - 1) for x in filtered_req]) / tt\n",
        "        throughput_prefill_lo_cur = np.sum([x.input_size if (\"lo:\" in x.id) else 0 for x in filtered_req_lo]) / tt\n",
        "        throughput_decode_lo_cur = np.sum([max(0, x.output_size - x.output_size_remaining - 1) if (\"lo:\" in x.id) else 0 for x in filtered_req_lo]) / tt\n",
        "        throughput_prefill_hi_cur = np.sum([x.input_size if (\"hi:\" in x.id) else 0 for x in filtered_req_hi]) / tt\n",
        "        throughput_decode_hi_cur = np.sum([max(0, x.output_size - x.output_size_remaining - 1) if (\"hi:\" in x.id) else 0 for x in filtered_req_hi]) / tt\n",
        "\n",
        "        # Calculate % of requests below latency target\n",
        "        latencies_lo = [(x.end_decode_time - x.arrival_time) / (x.output_size - x.output_size_remaining) for x in filtered_req_lo]\n",
        "        latencies_hi = [(x.end_decode_time - x.arrival_time) / (x.output_size - x.output_size_remaining) for x in filtered_req_hi]\n",
        "        pct_below_target_lo = (np.sum([1 if  x < target_latency_list_lo[0] else 0 for x in latencies_lo]) / len(latencies_lo)) * 100\n",
        "        pct_below_target_hi = (np.sum([1 if  x < target_latency_list_hi[0] else 0 for x in latencies_hi]) / len(latencies_hi)) * 100\n",
        "\n",
        "        # Store results for the current routing type\n",
        "        results[routing_type]['latency'].append(latency_cur)\n",
        "        results[routing_type]['latency_lo'].append(latency_cur_lo)\n",
        "        results[routing_type]['latency_hi'].append(latency_cur_hi)\n",
        "        results[routing_type]['throughput_prefill'].append(throughput_prefill_cur)\n",
        "        results[routing_type]['throughput_decode'].append(throughput_decode_cur)\n",
        "        results[routing_type]['throughput_prefill_lo'].append(throughput_prefill_lo_cur)\n",
        "        results[routing_type]['throughput_decode_lo'].append(throughput_decode_lo_cur)\n",
        "        results[routing_type]['throughput_prefill_hi'].append(throughput_prefill_hi_cur)\n",
        "        results[routing_type]['throughput_decode_hi'].append(throughput_decode_hi_cur)\n",
        "        results[routing_type]['ttft'].append(ttft_cur)\n",
        "        results[routing_type]['ttft_lo'].append(ttft_cur_lo)\n",
        "        results[routing_type]['ttft_hi'].append(ttft_cur_hi)\n",
        "        results[routing_type]['tpot'].append(tpot_cur)\n",
        "        results[routing_type]['tpot_lo'].append(tpot_cur_lo)\n",
        "        results[routing_type]['tpot_hi'].append(tpot_cur_hi)\n",
        "\n",
        "        results[routing_type]['recompute_cnt'].append(recompute_cur)\n",
        "        results[routing_type]['recompute_cnt_lo'].append(recompute_cur_lo)\n",
        "        results[routing_type]['recompute_cnt_hi'].append(recompute_cur_hi)\n",
        "\n",
        "        results[routing_type]['pct_below_latency_target_lo'].append(pct_below_target_lo)\n",
        "        results[routing_type]['pct_below_latency_target_hi'].append(pct_below_target_hi)\n",
        "\n",
        "        # Store pod distribution results\n",
        "        results[routing_type]['target_pods_lo'].append(Counter(target_pods_lo))\n",
        "        results[routing_type]['target_pods_hi'].append(Counter(target_pods_hi))\n",
        "\n",
        "        l1 = [np.sum(list(dict(x).values())) for x in results[routing_type]['target_pods_lo']]\n",
        "        l2 = [np.sum(list(dict(x).values())) for x in results[routing_type]['target_pods_hi']]\n",
        "\n",
        "        print(f'req count {[(l1[i], l2[i]) for i in range(len(l1))]}')\n",
        "\n",
        "        if routing_type == 'smart':\n",
        "            results[routing_type]['estimated_latency'].append(estimated_latency_cur)\n",
        "            results[routing_type]['estimated_latency_lo'].append(estimated_latency_cur_lo)\n",
        "            results[routing_type]['estimated_latency_hi'].append(estimated_latency_cur_hi)\n",
        "            print(f\"lo dist {Counter(target_pods_lo)} latency {latency_cur_lo} estimated_latency_lo {estimated_latency_cur_lo}\")\n",
        "            print(f\"hi dist {Counter(target_pods_hi)}  latency {latency_cur_hi} estimated_latency_hi {estimated_latency_cur_hi}\")\n",
        "        else:\n",
        "            print(f\"lo dist {Counter(target_pods_lo)} latency {latency_cur_lo} \")\n",
        "            print(f\"hi dist {Counter(target_pods_hi)}  latency {latency_cur_hi} \")\n",
        "\n",
        "        # Print the results for this qps\n",
        "        print(f'QPS: {rates_lo[i]} (lo), {rates_hi[i]} (hi)')\n",
        "        print(f'% of lo requests below target: {pct_below_target_lo}%')\n",
        "        print(f'% of hi requests below target: {pct_below_target_hi}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fzc38Agc0k9O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}